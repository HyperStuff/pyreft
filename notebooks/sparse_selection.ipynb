{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    sparsity_coeff: float = 0.1  # Reduced from 1.0\n",
    "    temp_start: float = 2.0  # Start with higher temperature\n",
    "    temp_end: float = 0.01  # End with lower temperature\n",
    "    epochs: int = 100\n",
    "    num_examples: int = 1000\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 0.001  # Reduced from 0.01\n",
    "    num_classes: int = 4\n",
    "    device: str = \"mps\"\n",
    "\n",
    "\n",
    "class CosineScheduler:\n",
    "    def __init__(self, start_value: float, end_value: float, num_steps: int):\n",
    "        self.start = start_value\n",
    "        self.end = end_value\n",
    "        self.num_steps = num_steps\n",
    "        self.current_step = 0\n",
    "        self.value = start_value\n",
    "\n",
    "    def step(self) -> float:\n",
    "        value = self.end + 0.5 * (self.start - self.end) * (\n",
    "            1 + math.cos((self.current_step / self.num_steps) * math.pi)\n",
    "        )\n",
    "        self.current_step = min(self.current_step + 1, self.num_steps)\n",
    "        self.value = value\n",
    "        return value\n",
    "\n",
    "\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "    def update_epoch_metrics(self, epoch_metrics: Dict[str, float]):\n",
    "        for key, value in epoch_metrics.items():\n",
    "            self.metrics[key].append(value)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        num_metrics = len(self.metrics)\n",
    "        fig, axes = plt.subplots(1, num_metrics, figsize=(4 * num_metrics, 4))\n",
    "\n",
    "        for ax, (metric_name, values) in zip(axes, self.metrics.items()):\n",
    "            ax.plot(values)\n",
    "            ax.set_title(metric_name.replace(\"_\", \" \").title())\n",
    "            ax.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.metrics = MetricsTracker()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        # Generate random points in the feature space\n",
    "        data = torch.randn(self.config.num_examples, 16) * 2.0\n",
    "\n",
    "        # Centers much closer together in a smaller square\n",
    "        centers = torch.tensor(\n",
    "            [\n",
    "                [0.7, 0.7],  # Class 0 center\n",
    "                [-0.7, 0.7],  # Class 1 center\n",
    "                [-0.7, -0.7],  # Class 2 center\n",
    "                [0.7, -0.7],  # Class 3 center\n",
    "            ],\n",
    "            dtype=torch.float,\n",
    "        )\n",
    "        # Much bigger radius to force lots of overlaps\n",
    "        radii = torch.ones(self.config.num_classes) * 1.8\n",
    "\n",
    "        # For each point, determine which circles it falls into\n",
    "        distances = torch.cdist(data[:, :2], centers)\n",
    "        labels = (distances <= radii).float()\n",
    "\n",
    "        # Let's verify we got all types of overlaps before proceeding\n",
    "        num_labels = labels.sum(dim=1)\n",
    "        for i in range(self.config.num_classes + 1):\n",
    "            count = (num_labels == i).sum().item()\n",
    "            print(f\"Points in {i} regions: {count}\")\n",
    "\n",
    "        # Create stratified split indices\n",
    "        all_indices = torch.arange(len(data))\n",
    "        holdout_indices = []\n",
    "\n",
    "        # Sample proportionally from each label count group\n",
    "        for n in torch.unique(num_labels):\n",
    "            group_indices = torch.where(num_labels == n)[0]\n",
    "            group_holdout_size = int(0.1 * len(group_indices))\n",
    "            holdout_indices.extend(\n",
    "                group_indices[\n",
    "                    torch.randperm(len(group_indices))[:group_holdout_size]\n",
    "                ].tolist()\n",
    "            )\n",
    "\n",
    "        holdout_indices = torch.tensor(holdout_indices)\n",
    "        train_indices = torch.tensor(\n",
    "            [i for i in all_indices if i not in holdout_indices]\n",
    "        )\n",
    "\n",
    "        # Create datasets using indices\n",
    "        train_dataset = TensorDataset(data[train_indices], labels[train_indices])\n",
    "        holdout_dataset = TensorDataset(data[holdout_indices], labels[holdout_indices])\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=self.config.batch_size, shuffle=True\n",
    "        )\n",
    "        holdout_loader = DataLoader(\n",
    "            holdout_dataset, batch_size=len(holdout_indices), shuffle=False\n",
    "        )\n",
    "\n",
    "        return train_loader, holdout_loader\n",
    "\n",
    "    def create_model(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, self.config.num_classes),\n",
    "        ).to(self.config.device)\n",
    "\n",
    "    def compute_losses(\n",
    "        self, output: torch.Tensor, target: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        # BCE loss for set membership probabilities\n",
    "        bce_loss = F.binary_cross_entropy(output, target)\n",
    "\n",
    "        # Sparsity loss - encourage fewer set memberships overall\n",
    "        sparsity_loss = self.config.sparsity_coeff * torch.mean(output)\n",
    "\n",
    "        # For metrics, we care about set membership accuracy\n",
    "        with torch.no_grad():\n",
    "            high_match = (torch.abs(output - target) < 0.2).float().mean().item()\n",
    "            exact_match = (\n",
    "                (torch.abs(output - target) < 0.2).all(dim=1).float().mean().item()\n",
    "            )\n",
    "\n",
    "        total_loss = bce_loss + sparsity_loss\n",
    "\n",
    "        metrics = {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"bce_loss\": bce_loss.item(),\n",
    "            \"sparsity_loss\": sparsity_loss.item(),\n",
    "            \"high_match\": high_match,\n",
    "            \"exact_match\": exact_match,\n",
    "        }\n",
    "\n",
    "        return total_loss, metrics\n",
    "\n",
    "    def _forward(self, network: nn.Module, inputs: torch.Tensor):\n",
    "        if network.training:\n",
    "            temp = self.temp_scheduler.step()\n",
    "        else:\n",
    "            temp = self.temp_scheduler.value\n",
    "        output = torch.sigmoid(network(inputs.to(self.config.device)) / temp)\n",
    "        return output\n",
    "\n",
    "    def train_step(\n",
    "        self,\n",
    "        batch_data: torch.Tensor,\n",
    "        batch_preds: torch.Tensor,\n",
    "        network: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "    ) -> Dict[str, float]:\n",
    "        temp = self.temp_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = self._forward(network, batch_data)\n",
    "        loss, metrics = self.compute_losses(output, batch_preds.to(self.config.device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics[\"temperature\"] = temp\n",
    "        return metrics\n",
    "\n",
    "    def plot_output_vectors(\n",
    "        self, network: torch.nn.Module, holdout_loader: torch.utils.data.DataLoader\n",
    "    ):\n",
    "        # Get all holdout data\n",
    "        holdout_data, holdout_labels = next(iter(holdout_loader))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Count how many regions each point belongs to\n",
    "            num_labels = holdout_labels.sum(dim=1)\n",
    "            outputs = self._forward(network, holdout_data)\n",
    "\n",
    "            # Create subplots - one for each possible number of overlapping regions (0 to num_classes)\n",
    "            fig, axes = plt.subplots(\n",
    "                1,\n",
    "                self.config.num_classes + 1,\n",
    "                figsize=(4 * (self.config.num_classes + 1), 4),\n",
    "            )\n",
    "\n",
    "            # Plot one example for each possible number of overlaps\n",
    "            for num_regions in range(self.config.num_classes + 1):\n",
    "                ax = axes[num_regions]\n",
    "                examples = torch.where(num_labels == num_regions)[0]\n",
    "\n",
    "                if len(examples) > 0:\n",
    "                    idx = examples[0]  # Take first example of this type\n",
    "                    # Plot vector components\n",
    "                    x = np.arange(len(outputs[idx]))\n",
    "                    ax.bar(x, outputs[idx].cpu().numpy())\n",
    "\n",
    "                    # Show which regions this point belongs to\n",
    "                    active_classes = (\n",
    "                        torch.where(holdout_labels[idx] > 0)[0].cpu().numpy()\n",
    "                    )\n",
    "                    classes_str = (\n",
    "                        f\"Regions: {', '.join(str(c) for c in active_classes)}\"\n",
    "                        if len(active_classes) > 0\n",
    "                        else \"No regions\"\n",
    "                    )\n",
    "\n",
    "                    ax.set_title(\n",
    "                        f\"In {num_regions} region{'s' if num_regions != 1 else ''}\\n{classes_str}\"\n",
    "                    )\n",
    "                else:\n",
    "                    ax.set_title(f\"No points found\\nin {num_regions} regions\")\n",
    "\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xlabel(\"Vector Component\")\n",
    "                ax.set_ylabel(\"Magnitude\" if num_regions == 0 else \"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self):\n",
    "        train_loader, holdout_loader = self.create_dataset()\n",
    "        network = self.create_model()\n",
    "        optimizer = torch.optim.SGD(network.parameters(), lr=self.config.learning_rate)\n",
    "\n",
    "        self.temp_scheduler = CosineScheduler(\n",
    "            self.config.temp_start,\n",
    "            self.config.temp_end,\n",
    "            len(train_loader) * self.config.epochs,\n",
    "        )\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            epoch_metrics = defaultdict(list)\n",
    "\n",
    "            for i, (batch_data, batch_preds) in enumerate(train_loader):\n",
    "                step_metrics = self.train_step(\n",
    "                    batch_data, batch_preds, network, optimizer\n",
    "                )\n",
    "\n",
    "                for key, value in step_metrics.items():\n",
    "                    epoch_metrics[key].append(value)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    metrics_str = [f\"Epoch {epoch}, Step {i}\"]\n",
    "                    for key, value in step_metrics.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            metrics_str.append(f\"{key}: {value:.4f}\")\n",
    "                        else:\n",
    "                            metrics_str.append(f\"{key}: {value}\")\n",
    "                    print(\", \".join(metrics_str))\n",
    "\n",
    "            # Average metrics for the epoch\n",
    "            avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n",
    "            self.metrics.update_epoch_metrics(avg_metrics)\n",
    "\n",
    "        self.metrics.plot_metrics()\n",
    "        self.plot_output_vectors(network, holdout_loader)\n",
    "        return network, self.metrics.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig(\n",
    "    sparsity_coeff=0.0,\n",
    "    epochs=100,\n",
    "    num_classes=4,\n",
    "    device=\"mps\",\n",
    "    learning_rate=1e-2,\n",
    "    temp_start=1.0,\n",
    "    temp_end=1e-2\n",
    ")\n",
    "trainer = Trainer(config)\n",
    "model, metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReconstructionConfig(TrainingConfig):\n",
    "    input_dim: int = 128\n",
    "    latent_dim: int = 16\n",
    "    double_well_coeff: float = 0.1\n",
    "\n",
    "\n",
    "class AutoencoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # Much narrower\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, temp=1.0):\n",
    "        h = self.encoder(x)\n",
    "        z = torch.sigmoid(h / temp)\n",
    "        out = self.decoder(z)\n",
    "        return out, z\n",
    "\n",
    "\n",
    "class ReconstructionTrainer(Trainer):\n",
    "    def create_dataset(self):\n",
    "        # Generate random noise\n",
    "        data = torch.randn(10000, self.config.input_dim)\n",
    "\n",
    "        # Normalize each vector to unit norm\n",
    "        data = F.normalize(data, p=2, dim=1)\n",
    "\n",
    "        # Create train/holdout splits\n",
    "        indices = torch.randperm(len(data))\n",
    "        split = int(0.9 * len(data))\n",
    "\n",
    "        train_dataset = TensorDataset(data[indices[:split]])\n",
    "        holdout_dataset = TensorDataset(data[indices[split:]])\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        holdout_loader = DataLoader(\n",
    "            holdout_dataset, batch_size=len(holdout_dataset), shuffle=False\n",
    "        )\n",
    "\n",
    "        return train_loader, holdout_loader\n",
    "\n",
    "    def create_model(self):\n",
    "        return AutoencoderModel(\n",
    "            input_dim=self.config.input_dim, latent_dim=self.config.latent_dim\n",
    "        ).to(self.config.device)\n",
    "\n",
    "    def compute_losses(self, output, target, latent):\n",
    "        recon_loss = F.mse_loss(output, target)\n",
    "        well_loss = self.config.double_well_coeff * torch.mean(latent * (1 - latent))\n",
    "\n",
    "        total_loss = recon_loss + well_loss\n",
    "\n",
    "        metrics = {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"recon_loss\": recon_loss.item(),\n",
    "            \"well_loss\": well_loss.item(),\n",
    "            \"sparsity\": ((latent < 0.1).float().mean()).item(),\n",
    "            \"binary_ratio\": ((latent < 0.1).float() + (latent > 0.9).float())\n",
    "            .mean()\n",
    "            .item(),\n",
    "        }\n",
    "\n",
    "        return total_loss, metrics\n",
    "\n",
    "    def train_step(self, batch_data, network, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, latent = network(\n",
    "            batch_data[0].to(self.config.device), self.temp_scheduler.step()\n",
    "        )\n",
    "\n",
    "        loss, metrics = self.compute_losses(\n",
    "            output, batch_data[0].to(self.config.device), latent\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics[\"temperature\"] = self.temp_scheduler.value\n",
    "        return metrics\n",
    "\n",
    "    def plot_output_vectors(self, network, holdout_loader):\n",
    "        data = next(iter(holdout_loader))[0]\n",
    "        with torch.no_grad():\n",
    "            recon, latent = network(\n",
    "                data.to(self.config.device), self.temp_scheduler.value\n",
    "            )\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Plot latent code distributions\n",
    "        plt.subplot(131)\n",
    "        plt.hist(latent.cpu().numpy().flatten(), bins=50)\n",
    "        plt.title(\"Latent Code Distribution\")\n",
    "        plt.xlabel(\"Latent Value\")\n",
    "        plt.ylabel(\"Count\")\n",
    "\n",
    "        # Plot average activation per latent dimension\n",
    "        plt.subplot(132)\n",
    "        mean_activations = latent.mean(0).cpu()\n",
    "        plt.bar(range(len(mean_activations)), mean_activations)\n",
    "        plt.title(\"Average Activation Per Latent Dim\")\n",
    "        plt.xlabel(\"Latent Dimension\")\n",
    "        plt.ylabel(\"Mean Activation\")\n",
    "\n",
    "        # Plot latent codes for a few examples\n",
    "        plt.subplot(133)\n",
    "        num_examples = 10\n",
    "        plt.imshow(latent[:num_examples].cpu().T, aspect=\"auto\")\n",
    "        plt.title(f\"Latent Codes (first {num_examples} examples)\")\n",
    "        plt.xlabel(\"Example\")\n",
    "        plt.ylabel(\"Latent Dimension\")\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def train(self):\n",
    "        train_loader, holdout_loader = self.create_dataset()\n",
    "        network = self.create_model()\n",
    "        optimizer = torch.optim.Adam(network.parameters(), lr=self.config.learning_rate)\n",
    "\n",
    "        self.temp_scheduler = CosineScheduler(\n",
    "            self.config.temp_start,\n",
    "            self.config.temp_end,\n",
    "            len(train_loader) * self.config.epochs,\n",
    "        )\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            epoch_metrics = defaultdict(list)\n",
    "\n",
    "            for i, batch_data in enumerate(train_loader):\n",
    "                step_metrics = self.train_step(batch_data, network, optimizer)\n",
    "\n",
    "                for key, value in step_metrics.items():\n",
    "                    epoch_metrics[key].append(value)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    metrics_str = [f\"Epoch {epoch}, Step {i}\"]\n",
    "                    for key, value in step_metrics.items():\n",
    "                        metrics_str.append(f\"{key}: {value:.4f}\")\n",
    "                    print(\", \".join(metrics_str))\n",
    "\n",
    "            avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n",
    "            self.metrics.update_epoch_metrics(avg_metrics)\n",
    "\n",
    "        self.metrics.plot_metrics()\n",
    "        self.plot_output_vectors(network, holdout_loader)\n",
    "        return network, self.metrics.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ReconstructionConfig(\n",
    "    input_dim=128,\n",
    "    latent_dim=4,\n",
    "    double_well_coeff=1.0,\n",
    "    epochs=50,\n",
    "    device=\"mps\",\n",
    "    learning_rate=1e-3,\n",
    "    temp_start=1.0,\n",
    "    temp_end=1.0,\n",
    ")\n",
    "trainer = ReconstructionTrainer(config)\n",
    "model, metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "import torch\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig(\n",
    "    [\n",
    "        {\n",
    "            \"layer\": 8,\n",
    "            \"component\": \"block_output\",\n",
    "            \"intervention_type\": pv.VanillaIntervention,\n",
    "        },\n",
    "        {\n",
    "            \"component\": \"wte.output\",\n",
    "            \"intervention_type\": pv.CollectIntervention,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base, counterfactual = pv_gpt2(\n",
    "    base=tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"),\n",
    "    sources=[tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"), None],\n",
    "    # sources=[tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")],\n",
    "    unit_locations={\"sources->base\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
