defaults:
  - _self_
  - task: base
  - override hydra/launcher: ray_jobs

hydra:
  launcher:
    poll_jobs: false
    entrypoint_num_gpus: 1

# Model settings
model:
  name: yahma/llama-7b-hf
  dtype: bfloat16
  max_length: 512
  embed_dim: 4096
  num_selection_attn_heads: 1
  start_temperature: 1.0
  end_temperature: 1e-3
  do_token_selection: true
  use_attn_weights: false
  scheduler: linear
  discretization_strategy: binary_concrete

# Training settings
training:
  train_dataset_key: null
  seed: 42
  epochs: 1
  max_steps: -1
  batch_size: 4
  eval_batch_size: 16
  gradient_accumulation_steps: 8
  learning_rate: 5e-3
  schedule: "linear"
  warmup_ratio: 0.0
  weight_decay: 0.0
  dropout: 0.0
  logging_steps: 1
  eval_strategy: epoch
  eval_steps: 100
  entropy_loss_weight: 1.0

# Intervention settings
intervention:
  type: "QuasiProjectiveIntervention"
  layers: "2;10;18;26"
  dict_size: 64
  low_rank_dimension: 8
  top_k_parameter: 8
  lambda_parameter: 0.1
  epsilon: 1e-6
  importance_power: -2
  selection_mechanism: "dynamic"
  scoring_dimension: 64
  orthogonal_init: true
  hat_matrix: false
  return_penalty: false
  add_bias: false
  share_weights: true
  position: "f2048"
  num_heads: 1
  act_fn: null

# LoRA settings
lora:
  use_lora: false
  disable_reft: false
  rank: 8
  alpha: 32
  modules: "o_proj"
  layers: "2;10;18;26"

# Logging settings
logging:
  is_wandb: true
  wandb_name: null
  wandb_proj: "HyperReFT"
  wandb_entity: "sidharth-baskaran-georgia-institute-of-technology"
  output_dir: "assets/official_results"
  save_model: false

generation:
  greedy_decoding: false
  temperature: null
  top_k: null
  top_p: null
