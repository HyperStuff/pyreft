defaults:
  - _self_
  - task: base

# Model settings
model:
  name: "yahma/llama-7b-hf"
  dtype: "bfloat16"
  max_length: 512

# Training settings
training:
  seed: 42
  epochs: 1
  batch_size: 4
  eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 5e-3
  schedule: "linear"
  warmup_ratio: 0.0
  weight_decay: 0.0
  dropout: 0.0
  logging_steps: 1

# Intervention settings
intervention:
  type: "LoreftIntervention"
  layers: "2;10;18;26"
  rank: 8
  position: "f1+l1"
  act_fn: null
  add_bias: false
  share_weights: false

# LoRA settings
lora:
  use_lora: false
  disable_reft: false
  rank: 8
  alpha: 32
  modules: "o_proj"
  layers: "2;10;18;26"

# Logging settings
logging:
  is_wandb: false
  wandb_name: "reft"
  wandb_dir: "wandb"
  wandb_proj: "MyReFT"
  output_dir: "./official_results"
  save_model: false